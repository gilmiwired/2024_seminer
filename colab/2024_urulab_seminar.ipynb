{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6FdeTpZh3O3",
        "outputId": "c0b09053-eb0a-490a-eab1-e4fb72380b90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Nov 26 03:33:24 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fashion-MNISTデータセットを用いたUNetベースの拡散モデルのトレーニングと、訓練済みモデルを使用した画像の生成"
      ],
      "metadata": {
        "id": "U6_4TVjE61Le"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYqU9tUEKB8L"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "img_size = 28         # 入力画像のサイズ\n",
        "batch_size = 128      # バッチサイズ\n",
        "num_timesteps = 1000  # 拡散過程のタイムステップ数\n",
        "epochs = 120           # エポック数\n",
        "lr = 1e-3             # 学習率\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # デバイスの設定\n",
        "\n",
        "model_dir = 'saved_model'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# 画像をグリッド状に表示する関数\n",
        "def show_images(images, rows=2, cols=10):\n",
        "    fig = plt.figure(figsize=(cols, rows))\n",
        "    i = 0\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            if i >= len(images):\n",
        "                break\n",
        "            ax = fig.add_subplot(rows, cols, i + 1)\n",
        "            ax.imshow(images[i].cpu().numpy().squeeze(), cmap='gray')  # 画像を表示\n",
        "            ax.axis('off')  # 軸を非表示に\n",
        "            i += 1\n",
        "    plt.show()\n",
        "\n",
        "# 単一の時間インデックスに対する位置エンコーディングを計算する関数\n",
        "def _pos_encoding(time_idx, output_dim, device='cpu'):\n",
        "    t, D = time_idx, output_dim\n",
        "    v = torch.zeros(D, device=device)\n",
        "    i = torch.arange(0, D, device=device)\n",
        "    div_term = torch.exp(i / D * math.log(10000))\n",
        "    v[0::2] = torch.sin(t / div_term[0::2])  # 偶数インデックスにサイン関数を適用\n",
        "    v[1::2] = torch.cos(t / div_term[1::2])  # 奇数インデックスにコサイン関数を適用\n",
        "    return v\n",
        "\n",
        "# バッチ内の各タイムステップに対する位置エンコーディングを計算する関数\n",
        "def pos_encoding(timesteps, output_dim, device='cpu'):\n",
        "    batch_size = len(timesteps)\n",
        "    device = timesteps.device\n",
        "    v = torch.zeros(batch_size, output_dim, device=device)\n",
        "    for i in range(batch_size):\n",
        "        v[i] = _pos_encoding(timesteps[i], output_dim, device)\n",
        "    return v\n",
        "\n",
        "# 時間埋め込みを含む畳み込みブロックの定義\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_embed_dim):\n",
        "        super().__init__()\n",
        "        # 畳み込み層の定義\n",
        "        self.convs = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),  # 1つ目の畳み込み層\n",
        "            nn.BatchNorm2d(out_ch),                  # バッチ正規化\n",
        "            nn.ReLU(),                               # 活性化関数\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1), # 2つ目の畳み込み層\n",
        "            nn.BatchNorm2d(out_ch),                  # バッチ正規化\n",
        "            nn.ReLU()                                # 活性化関数\n",
        "        )\n",
        "        # 時間埋め込み用のMLP\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(time_embed_dim, in_ch),        # 時間埋め込みをin_ch次元に変換\n",
        "            nn.ReLU(),                               # 活性化関数\n",
        "            nn.Linear(in_ch, in_ch)                  # 再度in_ch次元に変換\n",
        "        )\n",
        "\n",
        "    def forward(self, x, v):\n",
        "        N, C, _, _ = x.shape\n",
        "        # 時間埋め込みをMLPに通す\n",
        "        v = self.mlp(v)\n",
        "        v = v.view(N, C, 1, 1)  # xの形状に合わせてリシェイプ\n",
        "        # xに時間埋め込みを加えてから畳み込み層に通す\n",
        "        y = self.convs(x + v)\n",
        "        return y\n",
        "\n",
        "# UNetモデルの定義\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=1, time_embed_dim=100):\n",
        "        super().__init__()\n",
        "        self.time_embed_dim = time_embed_dim\n",
        "        # ダウンサンプリングパス\n",
        "        self.down1 = ConvBlock(in_ch, 64, time_embed_dim)\n",
        "        self.down2 = ConvBlock(64, 128, time_embed_dim)\n",
        "        # ボトルネック\n",
        "        self.bot1 = ConvBlock(128, 256, time_embed_dim)\n",
        "        # アップサンプリングパス\n",
        "        self.up2 = ConvBlock(128 + 256, 128, time_embed_dim)\n",
        "        self.up1 = ConvBlock(128 + 64, 64, time_embed_dim)\n",
        "        # 出力層\n",
        "        self.out = nn.Conv2d(64, in_ch, 1)\n",
        "        # プーリングとアップサンプリング\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "\n",
        "    def forward(self, x, timesteps):\n",
        "        # タイムステップに対する位置エンコーディングを取得\n",
        "        v = pos_encoding(timesteps, self.time_embed_dim, x.device)\n",
        "        # ダウンサンプリングパス\n",
        "        x1 = self.down1(x, v)   # 最初のダウンサンプリングブロック\n",
        "        x = self.maxpool(x1)    # マックスプーリング\n",
        "        x2 = self.down2(x, v)   # 2番目のダウンサンプリングブロック\n",
        "        x = self.maxpool(x2)    # マックスプーリング\n",
        "        # ボトルネック\n",
        "        x = self.bot1(x, v)\n",
        "        # アップサンプリングパス\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, x2], dim=1)  # スキップコネクションを結合\n",
        "        x = self.up2(x, v)             # 最初のアップサンプリングブロック\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, x1], dim=1)  # スキップコネクションを結合\n",
        "        x = self.up1(x, v)             # 2番目のアップサンプリングブロック\n",
        "        # 出力層\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "# 拡散モデル（Diffusion Model）の定義\n",
        "class Diffuser:\n",
        "    def __init__(self, num_timesteps=1000, beta_start=0.0001, beta_end=0.02, device='cpu'):\n",
        "        self.num_timesteps = num_timesteps\n",
        "        self.device = device\n",
        "        # β（ノイズスケジュール）の線形スケジュールを定義\n",
        "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps, device=device)\n",
        "        # αを計算（α = 1 - β）\n",
        "        self.alphas = 1 - self.betas\n",
        "        # 累積積 ᾱ を計算（ᾱ_t = ∏_{s=1}^t α_s）\n",
        "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
        "\n",
        "    # タイムステップtで画像にノイズを追加する関数\n",
        "    def add_noise(self, x_0, t):\n",
        "        \"\"\"\n",
        "        オリジナルの画像 x_0 に対して、タイムステップ t でノイズを追加した画像 x_t を生成する。\n",
        "        \"\"\"\n",
        "        T = self.num_timesteps\n",
        "        assert (t >= 1).all() and (t <= T).all()\n",
        "        t_idx = t - 1  # alpha_bars[0] は t=1 に対応\n",
        "        alpha_bar = self.alpha_bars[t_idx]\n",
        "        N = alpha_bar.size(0)\n",
        "        alpha_bar = alpha_bar.view(N, 1, 1, 1)\n",
        "        # 標準正規分布からノイズを生成\n",
        "        noise = torch.randn_like(x_0, device=self.device)\n",
        "        # ノイズを追加した画像 x_t を計算\n",
        "        x_t = torch.sqrt(alpha_bar) * x_0 + torch.sqrt(1 - alpha_bar) * noise\n",
        "        return x_t, noise\n",
        "\n",
        "    # タイムステップtで画像をデノイズする関数（逆拡散過程）\n",
        "    def denoise(self, model, x, t):\n",
        "        \"\"\"\n",
        "        モデルを使用して、ノイズの入った画像 x_t をタイムステップ t から x_{t-1} にデノイズする。\n",
        "        \"\"\"\n",
        "        T = self.num_timesteps\n",
        "        assert (t >= 1).all() and (t <= T).all()\n",
        "        t_idx = t - 1\n",
        "        alpha = self.alphas[t_idx]\n",
        "        alpha_bar = self.alpha_bars[t_idx]\n",
        "        # 前のタイムステップの ᾱ を取得\n",
        "        t_idx_prev = t_idx - 1\n",
        "        t_idx_prev = t_idx_prev.clamp(min=0)\n",
        "        alpha_bar_prev = self.alpha_bars[t_idx_prev]\n",
        "        # t=1 の場合、alpha_bar_prev を 1.0 に設定\n",
        "        alpha_bar_prev[t_idx == 0] = 1.0\n",
        "\n",
        "        N = alpha.size(0)\n",
        "        alpha = alpha.view(N, 1, 1, 1)\n",
        "        alpha_bar = alpha_bar.view(N, 1, 1, 1)\n",
        "        alpha_bar_prev = alpha_bar_prev.view(N, 1, 1, 1)\n",
        "\n",
        "        # モデルを使用してノイズ ε を予測\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            eps = model(x, t)\n",
        "        model.train()\n",
        "        # ランダムなノイズを生成（t=1 の場合はノイズを追加しない）\n",
        "        noise = torch.randn_like(x, device=self.device)\n",
        "        noise[t == 1] = 0\n",
        "        # 逆拡散過程の式に基づいて x_{t-1} を計算\n",
        "        mu = (x - ((1 - alpha) / torch.sqrt(1 - alpha_bar)) * eps) / torch.sqrt(alpha)\n",
        "        std = torch.sqrt((1 - alpha) * (1 - alpha_bar_prev) / (1 - alpha_bar))\n",
        "        x_prev = mu + noise * std\n",
        "        return x_prev\n",
        "\n",
        "    # テンソルをPIL画像に変換する関数\n",
        "    def reverse_to_img(self, x):\n",
        "        \"\"\"\n",
        "        モデルの出力を画像に変換するための関数。\n",
        "        \"\"\"\n",
        "        x = x.squeeze()\n",
        "        x = x * 0.5 + 0.5  # [-1,1] から [0,1] にスケーリング\n",
        "        x = x.clamp(0, 1)\n",
        "        x = x.cpu()\n",
        "        to_pil = transforms.ToPILImage()\n",
        "        return to_pil(x)\n",
        "\n",
        "    # 逆拡散プロセスを使用してサンプルを生成する関数\n",
        "    def sample(self, model, x_shape=(20, 1, 28, 28)):\n",
        "        \"\"\"\n",
        "        ノイズから開始し、モデルを使用してクリーンな画像を生成する。\n",
        "        \"\"\"\n",
        "        batch_size = x_shape[0]\n",
        "        # ランダムなノイズから開始\n",
        "        x = torch.randn(x_shape, device=self.device)\n",
        "        # 逆拡散過程をタイムステップ1まで繰り返す\n",
        "        for i in tqdm(range(self.num_timesteps, 0, -1)):\n",
        "            t = torch.tensor([i] * batch_size, device=self.device, dtype=torch.long)\n",
        "            x = self.denoise(model, x, t)\n",
        "        # テンソルを画像に変換\n",
        "        images = [self.reverse_to_img(x[i]) for i in range(batch_size)]\n",
        "        return images\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Fashion-MNISTデータセットの読み込みと前処理\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))  # 画像を [-1, 1] に正規化\n",
        "    ])\n",
        "    dataset = torchvision.datasets.FashionMNIST(root='./data', download=True, transform=preprocess)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # データセットからサンプル画像を表示\n",
        "    data_iter = iter(dataloader)\n",
        "    images, labels = next(data_iter)\n",
        "    show_images(images[:20], rows=2, cols=10)\n",
        "\n",
        "    # モデルとオプティマイザの定義\n",
        "    model = UNet(in_ch=1, time_embed_dim=100).to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "    diffuser = Diffuser(num_timesteps=num_timesteps, device=device)\n",
        "\n",
        "    # 学習ループ\n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for images, _ in tqdm(dataloader):\n",
        "            images = images.to(device)\n",
        "            N = images.size(0)\n",
        "            # 各バッチに対してランダムなタイムステップをサンプリング\n",
        "            t = torch.randint(1, num_timesteps + 1, (N,), device=device, dtype=torch.long)\n",
        "            # タイムステップ t で画像にノイズを追加\n",
        "            x_t, noise = diffuser.add_noise(images, t)\n",
        "            optimizer.zero_grad()\n",
        "            # モデルを使用して追加されたノイズを予測\n",
        "            noise_pred = model(x_t, t)\n",
        "            # 真のノイズと予測されたノイズとの間の損失を計算（平均二乗誤差）\n",
        "            loss = F.mse_loss(noise_pred, noise)\n",
        "            # バックプロパゲーション\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}')\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        torch.save(model.state_dict(), f'{model_dir}/model_epoch_{epoch+1}.pth')\n",
        "        print(f'エポック {epoch+1} でモデルを保存しました')\n",
        "\n",
        "    # 学習損失のプロット\n",
        "    plt.plot(range(1, epochs + 1), losses)\n",
        "    plt.xlabel('エポック')\n",
        "    plt.ylabel('損失')\n",
        "    plt.title('学習損失')\n",
        "    plt.show()\n",
        "\n",
        "    # 訓練されたモデルを使用して画像を生成\n",
        "    generated_images = diffuser.sample(model, x_shape=(20, 1, 28, 28))\n",
        "    # 生成された画像を表示\n",
        "    show_images(torch.stack([transforms.ToTensor()(img) for img in generated_images]), rows=2, cols=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "エポックごとの生成結果を比較する\n"
      ],
      "metadata": {
        "id": "CDHSvrMw7viB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from torchvision import transforms\n",
        "\n",
        "# デバイスの設定（GPUが利用可能ならGPUを、そうでなければCPUを使用）\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 生成画像を保存するディレクトリを設定し、存在しない場合は作成\n",
        "output_dir = 'saved_image'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def generate_and_save_images(model_path, diffuser, device, output_dir, num_images=20):\n",
        "    \"\"\"\n",
        "    指定されたモデルをロードし、画像を生成して保存する関数。\n",
        "\n",
        "    Args:\n",
        "        model_path (str): 保存されたモデルのファイルパス。\n",
        "        diffuser (Diffuser): 拡散モデルのインスタンス。\n",
        "        device (str): 使用するデバイス（'cuda' または 'cpu'）。\n",
        "        output_dir (str): 生成画像を保存するディレクトリ。\n",
        "        num_images (int): 生成する画像の枚数（デフォルトは20）。\n",
        "    \"\"\"\n",
        "    # モデルのインスタンスを新たに作成\n",
        "    model = UNet(in_ch=1, time_embed_dim=100).to(device)\n",
        "\n",
        "    # モデルの重みをロード\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()  # モデルを評価モードに設定\n",
        "\n",
        "    # 画像を生成\n",
        "    generated_images = diffuser.sample(model, x_shape=(num_images, 1, 28, 28))\n",
        "\n",
        "    # モデルファイル名からエポック番号を抽出\n",
        "    epoch_num = os.path.splitext(os.path.basename(model_path))[0].split('_')[-1]\n",
        "\n",
        "    # 生成された各画像を保存\n",
        "    for idx, img in enumerate(generated_images):\n",
        "        # 画像の保存パスを作成（例: epoch_10_img_1.png）\n",
        "        save_path = os.path.join(output_dir, f'epoch_{epoch_num}_img_{idx+1}.png')\n",
        "        img.save(save_path)\n",
        "    print(f'{model_path} から画像を生成し、{output_dir} に保存しました')\n",
        "\n",
        "    # 生成された画像をテンソルに変換して表示\n",
        "    tensor_images = torch.stack([transforms.ToTensor()(img) for img in generated_images])\n",
        "    show_images(tensor_images, rows=2, cols=10)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 拡散モデルのインスタンスを作成\n",
        "    diffuser = Diffuser(num_timesteps=1000, device=device)\n",
        "\n",
        "    # 特定のディレクトリから保存されたモデルファイルをリストアップ\n",
        "    saved_model_files = [f for f in os.listdir(model_dir) if f.startswith('model_epoch_') and f.endswith('.pth')]\n",
        "    # エポック番号でソート（昇順）\n",
        "    saved_model_files.sort(key=lambda x: int(x.split('_')[-1].split('.pth')[0]))\n",
        "\n",
        "    # 各保存されたモデルファイルに対して画像を生成・保存\n",
        "    for model_file in saved_model_files:\n",
        "        model_path = os.path.join(model_dir, model_file)\n",
        "        generate_and_save_images(model_path, diffuser, device, output_dir, num_images=20)\n"
      ],
      "metadata": {
        "id": "QOIgPP2srYYn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}